# 多模态大模型入门 (VLM Intro)

> 💡 **学习指南**：本章节无需深厚的计算机视觉背景，通过交互式演示带你理解 AI 是如何拥有“眼睛”的。我们将揭秘 GPT-4V、Qwen-VL 等模型背后的核心原理。

<VlmQuickStartDemo />

## 0. 引言：从“读万卷书”到“行万里路”

在 [大语言模型入门](./llm-intro) 章节中，我们学习了计算机如何通过 Tokenization（分词）和 Transformer 理解文字。
但真实世界不仅有文字，还有图像、视频和声音。

**多模态大模型 (VLM, Vision-Language Model)** 的核心任务，就是打破感官的界限，让 AI 不仅能“读”，还能“看”。

它的本质工作可以总结为一句话：**把图像信号“翻译”成大模型能听懂的语言信号。**

---

## 1. 第一步：视觉翻译 (Visual Tokenization)

大模型（LLM）本质上是一个“文字接龙”机器，它只认识数字（Token ID）。要让它看懂图片，我们必须把图片也变成它能理解的数字序列。

这个过程主要由 **Vision Transformer (ViT)** 完成。**请注意：ViT 本身就是一个独立的、强大的深度学习模型**，你可以把它想象成 AI 的“视网膜”。

### 1.1 为什么是 Transformer？(ViT 详解)

在 ViT 出现之前，计算机看图主要靠 **CNN (卷积神经网络)**，它像一个放大镜，一点一点地扫描图片提取特征。但 CNN 有个局限：它很难理解“全局关系”（比如左上角的鸟和右下角的树有什么关系）。

**Transformer** 的核心优势在于**全局注意力 (Global Attention)**。它能同时看到整张图，并理解各个部分之间的关联。

但 Transformer 本来是处理文本（一维序列）的，怎么处理图片（二维矩阵）呢？

| NLP (文本处理) | CV (图像处理) |
| :--- | :--- |
| **句子** (Sentence) | **图片** (Image) |
| **单词** (Word) | **图片块** (Patch) |
| **词向量** (Embedding) | **特征向量** (Patch Feature) |

**ViT 的核心思想**：把一张图切成很多小块，然后把这些小块当成一个个单词，排成一句话喂给 Transformer。

#### 核心步骤拆解：

1.  **切块 (Patchify)**：
    就像把一张完整的拼图拆散。假设输入图片是 `224x224` 像素，我们设定每个 Patch 大小为 `16x16`。
    那么这张图就被切成了 (224/16) × (224/16) = 14 × 14 = 196 个小块。
    每个小块就是一个基础的视觉单词。

    <PatchifyDemo />

2.  **拉平与映射 (Linear Projection)**：
    每个 `16x16` 的彩色小块包含 16 × 16 × 3 (RGB) = 768 个像素点。
    我们把这 768 个点拉成一条直线（向量），然后通过一个线性层（矩阵乘法）把它压缩成固定长度的特征向量（比如 768 维或 1024 维）。
    *现在的状态：196 个向量。*

    <LinearProjectionDemo />

3.  **加上位置编码 (Positional Embedding)**：
    Transformer 是“无序”的。如果你把拼图打乱，它就不知道哪块是头，哪块是脚。
    所以，我们必须给每个向量“贴上号码牌”：这是第1行第1列，那是第3行第5列。
    这样模型就能记住图片的空间结构。

    <PositionalEmbeddingDemo />

4.  **自注意力机制 (Self-Attention)**：
    这是最神奇的一步。这 196 个 Patch 开始“开会”互相交流。
    *   **Patch A (猫耳朵)** 问：我是毛茸茸的三角形，谁跟我有关？
    *   **Patch B (猫脸)** 回答：我是圆圆的脸，我们可以拼成一只猫头！
    *   **Patch C (背景树)** 回答：我是绿色的，跟你们关系不大。
    通过层层计算，模型不仅识别出了孤立的特征，还理解了物体之间的**语义关系**。

    <AttentionDemo />

5.  **输出 (Output)**：
    最终，ViT 输出的是一串**富含语义的特征向量序列**。这串向量就是 LLM 后续要阅读的“图像文章”。

    <ViTOutputDemo />



---

## 2. 核心难题：跨界沟通 (Projection)

ViT 输出的向量虽然包含了图像信息，但它说的是“视觉方言”，LLM 的大脑只能听懂“文本方言”。
**Projector (投射器)** 就是这个翻译官，负责对齐这两种语言的维度和语义。

### 架构对比：三种流派

<ProjectorDemo />

#### 1. 简单粗暴派：Linear Projector (如 LLaVA)
*   **结构**：一个简单的全连接层 (MLP)。
*   **数学原理**：$Y = WX + b$。其中 $X$ 是视觉向量，$W$ 是训练好的权重矩阵。
*   **比喻**：**直译**。把视觉向量强行“拉伸”或“压缩”到和文本向量一样的维度。
*   **优点**：保留了最多的原始视觉信息，几乎没有信息损失。
*   **缺点**：Token 数量多。一张图可能产生 576 个 Token，LLM 处理起来比较累。

#### 2. 精细提取派：Q-Former (如 BLIP-2)
*   **结构**：一个小型的 Transformer，带有两组输入：一组是固定的“查询向量 (Queries)”，一组是图片特征。
*   **原理**：
    *   预设 32 个 Query（就像 32 个带着问题的记者）。
    *   这些记者进入图片特征的海洋里，寻找自己感兴趣的信息。
    *   最后只输出这 32 个记者采集到的精华摘要。
*   **比喻**：**意译/摘要**。不管原图多复杂，我都只给你总结出 32 句话。
*   **优点**：Token 数量极少（32个），LLM 跑得飞快。
*   **缺点**：信息压缩太狠，容易丢失细节（比如图片角落里的小字）。

#### 3. 注意力压缩派：C-Abstractor (如 Qwen-VL)
*   **结构**：在 Linear 和 Q-Former 之间取平衡。利用卷积或注意力机制将相邻的 Patch 合并。
*   **原理**：比如把 $2\times2$ 的 4 个 Patch 合并成 1 个。
*   **优点**：既减少了 Token 数量（降低计算量），又保留了足够的空间细节。

---

## 3. 进化之路：ViT + LLM

现在的多模态大模型（M-LLM）本质上就是：**给 LLM 装了一副眼镜**。

### 模型架构对比

让我们直观地对比一下传统 LLM 和 VLM 在架构上的区别。

<ModelArchitectureComparisonDemo />

### 模型解剖
一个标准的 LLaVA 架构模型由三部分物理连接而成：

1.  **Vision Encoder (ViT)**
    *   *来源*：通常借用已经训练好的模型（如 CLIP-ViT-L/14, SigLIP）。
    *   *状态*：在训练初期通常是**冻结 (Frozen)** 的，因为它们已经很会看图了。
2.  **Projector (Adapter)**
    *   *来源*：从零初始化。
    *   *状态*：**全程训练**。它是连接视觉和语言的关键枢纽。
3.  **LLM (Backbone)**
    *   *来源*：开源大模型（如 Vicuna, Qwen, Llama-3）。
    *   *状态*：在预训练阶段冻结，在微调阶段解冻。

### 视频也能看吗？
是的。对于模型来说，视频就是**一连串连续的图片**。
*   **抽帧**：每秒抽取 1 帧或 2 帧。
*   **堆叠**：把这 10 张图片的 Token 串起来，告诉 LLM：“这是第一帧，这是第二帧...”。
*   **时间编码**：有些高级模型会加上“时间戳 Token”，让 LLM 理解动作的先后顺序。

---

## 4. 训练揭秘：从对齐到对话 (Training Pipeline)

要把这三个零件（ViT, Projector, LLM）磨合好，通常需要两阶段训练。

### 阶段一：特征对齐 (Feature Alignment / Pre-training)
*   **目标**：让 Projector 学会翻译。此时 LLM 还不参与学习，只是充当裁判。
*   **做法**：
    *   **冻结**：ViT 和 LLM。
    *   **只训练**：Projector。
    *   **数据**：558K 对简单的 `<图片, 标题>` 数据 (CC3M, LAION)。
*   **过程**：
    输入一张“猫”的图 -> ViT -> Projector -> 得到向量 V。
    输入文字“一只猫” -> LLM -> 得到向量 T。
    **Loss**：强迫向量 V 和向量 T 尽可能相似。
*   **结果**：Projector 能够把图像特征转换成 LLM 能够理解的 Embedding 空间。

<FeatureAlignmentDemo />

### 阶段二：视觉指令微调 (Visual Instruction Tuning / SFT)
*   **目标**：让模型学会听指令，进行复杂对话。
*   **做法**：
    *   **冻结**：ViT (通常保持冻结，有些激进的训练会解冻)。
    *   **全量微调**：Projector + LLM。
    *   **数据**：150K+ 高质量的对话数据 (LLaVA-Instruct)。
        *   *User*: `<image>` 图中的男人穿什么颜色的衣服？
        *   *Assistant*: 他穿着一件蓝色的衬衫。
*   **结果**：LLM 学会了结合图片信息来回答用户的问题，而不仅仅是补全文字。

<VLMInferenceDemo />

---

## 5. 进阶：新模型的视觉 Trick (Advanced Tricks)

### 5.1 Qwen-VL 的创新：像人眼一样看 (Naive Dynamic Resolution)

传统的 ViT (如 CLIP) 有个大毛病：**强制缩放**。
不管你给它一张长长的手机截图，还是一张扁扁的全景照，它都会粗暴地把图片拉伸成 `224x224` 的正方形。
*   **后果**：文字变形看不清，小物体丢失。

**Qwen-VL** 引入了 **Naive Dynamic Resolution（动态分辨率）** 机制：
1.  **保持原比例**：图片是长条的，就按长条的切。
2.  **智能分块**：将大图切成多个 `224x224` 的子图（就像用手机拍全景时移动镜头）。
3.  **全局视角**：除了看局部细节，还会生成一张缩略图看整体布局。
这就好比人眼看东西：既能眯着眼看全貌，也能凑近了看细节，保证了高清图像的信息不丢失。

### 5.2 LLaVA-NeXT (LLaVA-1.6): AnyRes 技术

**LLaVA-NeXT** 采用了 **AnyRes (Any Resolution)** 技术，这是一种灵活的分辨率处理策略。
*   **网格切分**：它构建了一个包含不同长宽比的网格配置集合（如 1:1, 1:2, 2:1 等）。给定一张输入图像，模型会从集合中选择最匹配的网格配置。
*   **避免变形**：通过这种方式，尽可能减少因缩放导致的图像变形。
*   **全局与局部结合**：它也会同时输入一张调整大小后的全图（用于看整体）和切分后的局部图块（用于看细节），让 LLM 综合判断。

### 5.3 InternVL: 让眼睛变大 (Scaling Vision Encoder)

传统的 VLM 往往使用 CLIP-ViT-Large (约 300M 参数) 作为视觉编码器。
**InternVL (书生·万象)** 的思路很直接：**如果视力不好，那就换个更大的眼睛！**
*   它使用了一个高达 **60亿参数 (6B)** 的超大视觉编码器 (InternViT-6B)。
*   这使得模型在无需任何微调的情况下，光靠“眼睛”就能看懂非常复杂的视觉细节，甚至能做语义分割。

### 5.4 DeepSeek-VL & MiniCPM-V: 细节为王 (High-Res Tiling)

对于需要看清密集文字（OCR）或微小物体（如仪表盘读数）的场景，**DeepSeek-VL** 和 **MiniCPM-V** 采用了更激进的高清切片策略。
*   **混合视觉编码**：DeepSeek-VL 混合使用了负责语义理解的 SigLIP 和负责细节捕捉的 SAM (Segment Anything Model) 编码器，兼顾了“看得懂”和“看得清”。
*   **自适应切片**：MiniCPM-V 针对端侧设备优化，能够智能地将高清大图切分为多个小图输入，即使是 800万像素的图片也能在手机上被精准识别。

---

## 6. 总结

VLM 的奇迹在于它证明了**语义的统一性**。无论是像素（图像）还是字符（文本），在深度神经网络的高维空间里，最终都可以汇聚为统一的数学表示。

当你给 AI 发一张照片时，你其实是在发送一串它能“读懂”的数字诗篇。

---

## 7. 名词速查表 (Glossary)

| 名词 | 全称 | 解释 |
| :--- | :--- | :--- |
| **VLM** | Vision-Language Model | 多模态大模型。既能理解文本，又能理解图像（甚至视频）的 AI 模型。 |
| **ViT** | Vision Transformer | 视觉 Transformer。将图像切分为 Patch 并通过 Self-Attention 提取特征的模型，是 VLM 的“眼睛”。 |
| **Patch** | - | **图像块**。ViT 将图像切分成的固定大小的小方块（如 16x16 像素），相当于文本中的单词。 |
| **Projector** | - | **投射器/对齐层**。连接 ViT 和 LLM 的桥梁，负责将视觉特征向量转换为 LLM 能理解的文本向量维度。 |
| **Linear Projection** | - | **线性映射**。最简单的 Projector，通过一个矩阵乘法改变向量维度。 |
| **Q-Former** | Querying Transformer | 一种复杂的 Projector，使用可学习的 Query 向量从图像特征中提取关键信息。 |
| **Feature Alignment** | - | **特征对齐**。VLM 训练的第一阶段，目的是让 Projector 学会将图像特征映射到文本空间。 |
| **Visual Instruction Tuning** | - | **视觉指令微调**。VLM 训练的第二阶段，使用多模态对话数据让模型学会根据图像回答问题。 |
| **Resolution** | - | **分辨率**。图像的像素尺寸（如 224x224）。分辨率越高，看得越清，但计算量越大。 |
| **AnyRes** | Any Resolution | **任意分辨率**。一种能够灵活处理不同尺寸和长宽比图像的技术，避免图像变形。 |
| **OCR** | Optical Character Recognition | **光学字符识别**。从图像中提取文字的技术。现代 VLM 通常具备强大的 OCR 能力。 |

